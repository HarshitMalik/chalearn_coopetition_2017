import csvimport cPickle as pickleimport mathimport numpy as npimport randomimport scipyimport sysfrom scipy import statsfrom os import listdirfrom os.path import isfile, joinfrom sklearn import linear_modelfrom sklearn.svm import SVRfrom sklearn import ensemblefrom sklearn.metrics import mean_squared_errorfrom sklearn import preprocessing# SVM classes for the libsvm librarysys.path.append('./libsvm-3.17/python')from svmutil import *FACIAL_FEATURES = ["Pitch", "Yaw  ", "Roll ", "inBrL", "otBrL", "inBrR", "otBrR",                   "EyeOL", "EyeOR", "oLipH", "iLipH", "LipCDt"]TARGET_LABEL = "interview"def LoadTranscriptFile(transcription_file):  fp = open(transcription_file, "r")  transcripts = pickle.load(fp)  print "Number of transcripts:", len(transcripts)   return transcripts###########################  ML Related Functions ################def SplitTrainingTestSet(x, y, ind, ind_bar):  """Split the data into training and test sets."""  ytest = list();  ytrain = list();  xtest = list();  xtrain = list();  for i in ind:    xtrain.append(x[i])    ytrain.append(y[i])  for i in ind_bar:    xtest.append(x[i])    ytest.append(y[i])  return (ytrain,xtrain,ytest,xtest)def GetPartitionedIndices(n, ratio):  training_size = int(n * ratio)  randindices = random.sample(range(1, n), training_size)  ind = []  for i in randindices:    ind.append(i)  ind_bar = list(set(range(0, n)) - set(ind));  return ind, ind_bardef CombineFeatures(feature_dicts):  result = set(feature_dicts[0].keys())  for d in feature_dicts[1:]:    key_set = set(d.keys())    result &= key_set  print "Intersection size: " , len(result)  comb_features = {}  for k in result:    comb_features[k] = {}    for feature_dict in feature_dicts:      for f_key in feature_dict[k]:        comb_features[k][f_key] = feature_dict[k][f_key]  return comb_features    def GetFeatureAndLabelArraysFromDictionaries(D_features, D_labels):  X = []  y = list()  for k in sorted(D_features):    Xrow = list()    for f in sorted(D_features[k]):      Xrow.append(D_features[k][f])    X.append(Xrow)    y.append(D_labels[k])  print len(X)  print len(y)  #Xn = stats.zscore(X, axis=0)  return X, ydef AccuracyMetrics(ypredict, ytrue):  # Estimate correlation coefficients  corrval = np.corrcoef(np.array(ytrue), np.array(ypredict))  correlation = corrval[0][1]        # Estimate RMS Error  rms_error = math.sqrt(mean_squared_error(np.array(ytrue), np.array(ypredict)))        # Estimate Normalized RMS  datarange = np.amax(np.array(ytrue)) - np.amin(np.array(ytrue))  normalized_rms = rms_error/datarange  return correlation, rms_error, normalized_rmsdef main():  fp = open("./training_features.pkl", "r")  data = pickle.load(fp)  X = data['X']  y = data['y']  min_max_scaler = preprocessing.MinMaxScaler()  X = min_max_scaler.fit_transform(X)  ind, ind_bar = GetPartitionedIndices(len(y), 0.8)  (ytrain,xtrain,ytest,xtest) = SplitTrainingTestSet(X,y,ind,ind_bar)  print len(ytrain), len(xtrain), len(ytest), len(xtest)  method = "gb"  if method == "lasso":    lasso = linear_model.Lasso(alpha=0.001, fit_intercept=True,                               normalize=False, precompute='auto',                               copy_X=True, max_iter=1000, tol=0.0001,                               warm_start=False, positive=False)    lasso.fit(xtrain, ytrain)    yp = lasso.predict(xtest)  elif method == "svm":##    prob = svm_problem(ytrain, xtrain)##    param = svm_parameter('-s 3 -t 0 -c 0.01 -p 0.1 -e 0.1 -h')##    m = svm_train(prob, param)##    yp, p_acc, p_val = svm_predict(ytest, xtest, m)    svr = SVR(C=0.010, kernel='linear', epsilon=0.00001, tol=0.00001, verbose=True,              max_iter=100000)    svr.fit(xtrain, ytrain)    yp = svr.predict(xtest)  #################################  elif method == "logreg":    logistic = linear_model.LogisticRegression(penalty='l1', dual=False,                                               tol=0.0001, C=10.0,                                               fit_intercept=False,                                               intercept_scaling=1,                                               class_weight=None,                                               random_state=None)    logistic.fit(xtrain, ytrain)    yp = logistic.predict(xtest)  elif method == "gb":    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,          'learning_rate': 0.01, 'loss': 'ls'}    clf = ensemble.GradientBoostingRegressor(**params)    clf.fit(xtrain, ytrain)    yp = clf.predict(xtest)  #################################  elif method == "gp":    gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4,                                          thetaU=1e-1)    gp.fit(xtrain, ytrain)    yp, sigma2_p = gp.predict(xtest, eval_MSE=True)  for i in range(len(ytest)):    print yp[i], ytest[i]  corr, rms, norm_rms = AccuracyMetrics(yp, ytest)  print corr, rms, norm_rms###################################      if method == "lasso":##        lasso = linear_model.Lasso(alpha=0.05, fit_intercept=True, normalize=False, precompute='auto', copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False)##        lasso.fit(X_train, ytrain)##        yp = lasso.predict(X_test)##        nfeatures = len(X_train[0])##        if iter == 0:##          weights = [0.0] * nfeatures##        ##        for jj in range(len(weights)):##          weights[jj] = weights[jj] + (lasso.coef_[jj])/float(nIters)################################################if __name__ == "__main__":  main()